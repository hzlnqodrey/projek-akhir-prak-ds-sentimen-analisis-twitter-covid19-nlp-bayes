{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Projek Akhir Praktikum Data Science**\n",
    "\n",
    "> Analisis Sentimen Mengenai Vaksin COVID-19 Di Indonesia Menggunakan Metode Naive Bayes Classifier dan NLP Pada Sosial Media Twitter\n",
    "\n",
    "**Oleh Kelompok 2 :**\n",
    "1. Hazlan Muhammad Qodri (123190080)\n",
    "2. Elisia Dwi Rahayu (123190062)\n",
    "3. Shania Septika Inayasari (123190055)\n",
    "\n",
    "**Penjelasan Projek :**\n",
    "\n",
    "Adapun pada penelitian menekankan kepada sentimen masyarakat terhadap mengenai vaksin COVID-19. Proses analisisnya akan dilakukan berdasarkan tweet yang menyertakan tagar vaksin dan pencarian di twitter dengan keyword vaksin covid 19."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Scraping Data from Twitter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import csv\n",
    "import os\n",
    "from datetime import timedelta, datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Secret Env Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = os.environ.get('consumer_key')\n",
    "consumer_secret = os.environ.get('consumer_key_secret')\n",
    "access_token = os.environ.get('access_token')\n",
    "access_token_secret = os.environ.get('access_token_secret')\n",
    "bearer_token = os.environ.get('bearer_token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Failed On, 403 Forbidden\n",
      "453 - You currently have Essential access which includes access to Twitter API v2 endpoints only. If you need access to this endpoint, youâ€™ll need to apply for Elevated access via the Developer Portal. You can learn more here: https://developer.twitter.com/en/docs/twitter-api/getting-started/about-twitter-api#v2-access-leve\n"
     ]
    }
   ],
   "source": [
    "# token for twitter developers\n",
    "client = tweepy.Client(\n",
    "    bearer_token=bearer_token,\n",
    "    consumer_key=consumer_key,\n",
    "    consumer_secret=consumer_secret,\n",
    "    access_token=access_token,\n",
    "    access_token_secret=access_token_secret,\n",
    ")\n",
    "\n",
    "def scrap_tweets(search_words, date_since):\n",
    "    # Membuat Kolom Untuk Di Export Ke Excel\n",
    "    db_tweets = pd.DataFrame(columns=[\n",
    "        'username', 'acctdesc', 'location', 'following',\n",
    "        'totaltweets', 'usercreatedts', 'tweetcreatedts',\n",
    "        'retweetcount', 'text', 'hashtags', 'followers',\n",
    "    ])\n",
    "    print(db_tweets)\n",
    "\n",
    "    # Melakukan Query Pencarian Data Tweet Sesuai Kata Kunci Dan Tanggal\n",
    "    # tweets = client.search_recent_tweets(query=search_words,expansions=None, max_results=100, since_id=date_since)\n",
    "    tweet_list = []\n",
    "    for tweet in client.sea(q=\"iphone\", lang=\"en\"):\n",
    "        print(tweet.text)\n",
    "    print(tweet_list)\n",
    "\n",
    "    # Merubah Kumpulan Item Hasil Tweet Menjadi Kumpulan Data Dalam Array List\n",
    "    \n",
    "\n",
    "    # # Melakukan Perulangan Untuk Data Tweet Untuk Dimasukkan Kedalam CSV\n",
    "    for tweet in tweet_list:\n",
    "        username = tweet.user.screen_name\n",
    "        acctdesc = tweet.user.description\n",
    "        location = tweet.user.location\n",
    "        following = tweet.user.friends_count\n",
    "        followers = tweet.user.followers_count\n",
    "        totaltweets = tweet.user.statuses_count\n",
    "        usercreatedts = tweet.user.created_at\n",
    "        tweetcreatedts = tweet.created_at\n",
    "        retweetcount = tweet.retweet_count\n",
    "        hashtags = tweet.entities['hashtags']\n",
    "\n",
    "        try:\n",
    "            text = tweet.retweeted_status.full_text\n",
    "        except AttributeError:\n",
    "            text = tweet.full_text\n",
    "\n",
    "        # Membuat Array Kumpulan Data Sesuai Kolom Dan Memasukkan Kedalam Array Data Tweet\n",
    "        ith_tweet = [\n",
    "            username, acctdesc, location, following, followers, totaltweets,\n",
    "            usercreatedts, tweetcreatedts, retweetcount, text, hashtags\n",
    "        ]\n",
    "\n",
    "        db_tweets.loc[len(db_tweets)] = ith_tweet\n",
    "\n",
    "    # Export Data Kumpulan Tweet Ke File CSV\n",
    "    filename = 'covid_vaccine_tweets.csv'\n",
    "    db_tweets.to_csv(filename, index=False)\n",
    "    print('Scraping has completed!')\n",
    "\n",
    "\n",
    "# Date Config\n",
    "today = datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')\n",
    "last_week = datetime.utcnow() - timedelta(7)\n",
    "last_week = last_week.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Searching Word\n",
    "search_words = \"covid indonesia OR kasus covid OR #kasuscovid OR #vaksin OR #waspadacovid\"\n",
    "date_since = last_week\n",
    "\n",
    "scrap_tweets(search_words, date_since)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using SNSCRAPE\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "898a950986d43450680efc03f9903704e020e6e6b23d64c62a66308a081cc53c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
