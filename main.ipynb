{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Projek Akhir Praktikum Data Science**\n",
    "\n",
    "> Analisis Sentimen Mengenai Vaksin COVID-19 Di Indonesia Menggunakan Metode Naive Bayes Classifier dan NLP Pada Sosial Media Twitter\n",
    "\n",
    "**Oleh Kelompok 2 :**\n",
    "1. Hazlan Muhammad Qodri (123190080) @hzlnqodrey\n",
    "2. Elisia Dwi Rahayu (123190062) @elisiadwirahayu\n",
    "3. Shania Septika Inayasari (123190055) @shaniainayasari\n",
    "\n",
    "**Penjelasan Projek :**\n",
    "\n",
    "Adapun pada penelitian menekankan kepada sentimen masyarakat terhadap mengenai vaksin COVID-19. Proses analisisnya akan dilakukan berdasarkan tweet yang menyertakan tagar vaksin dan pencarian di twitter dengan keyword vaksin covid 19."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Scraping Data from Twitter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install snscrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get All Covid Sentiment Data from January 1st, 2020 until November 1st, 2022\n",
    "query = \"covid since:2020-01-01 until:2022-11-01 lang:id\"\n",
    "limit = 2000 # limit 50k rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "\n",
    "for tweet in sntwitter.TwitterSearchScraper(query).get_items():\n",
    "    if len(tweets) == limit:\n",
    "        break\n",
    "    else:\n",
    "        tweets.append([\n",
    "            tweet.date,\n",
    "            tweet.username,\n",
    "            tweet.content\n",
    "        ])\n",
    "\n",
    "filename = 'tweets_covid_dataset_2k_raw_noindex.csv'\n",
    "tweets_df = pd.DataFrame(tweets, columns=['Tanggal', 'Username', 'Text'])\n",
    "tweets_df.to_csv(filename, index=False)\n",
    "print('Scraping has completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Wrangling Data** (Preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tweet-preprocessor\n",
    "%pip install textblob\n",
    "%pip install wordcloud\n",
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessor as preproc\n",
    "from textblob import TextBlob\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import csv\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data from dataset\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/hzlnqodrey/projek-akhir-prak-ds-sentimen-analisis-twitter-covid19-nlp-bayes/main/data_csv/tweets_covid_dataset_50k_raw_noindex.csv', engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(labels=16413, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()\n",
    "data.isna().any()\n",
    "data.dropna(subset=['Username'])\n",
    "data.dropna(subset=['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['Tanggal'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Case Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Text'] = data['Text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Case Folding Result : \\n')\n",
    "data.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning overall\n",
    "def preprocessing_data(x):\n",
    "    return preproc.clean(x)\n",
    "\n",
    "data['Text'] = data['Text'].apply(preprocessing_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cleaning Result : \\n')\n",
    "data.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning remove_comments_special\n",
    "def remove_comments_special(text):\n",
    "    # remove tab, new line, and back slice\n",
    "    text = text.replace('\\\\t', \" \").replace('\\\\n', \" \").replace(\n",
    "        '\\\\u', \" \").replace('\\\\', \" \").replace('.', \" \")\n",
    "    # remove non ASCII (emoticon, chinese word, .etc)\n",
    "    text = text.encode('ascii', 'replace').decode('ascii')\n",
    "    # remove mention, link, hashtag\n",
    "    text = ' '.join(\n",
    "        re.sub(\"([@#][A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\", \" \", text).split())\n",
    "    # remove ascii decoded\n",
    "    text = ' '.join(\n",
    "        re.sub(\"amp; \", \" \", text).split())\n",
    "    text = ' '.join(\n",
    "        re.sub(\"lt; \", \" \", text).split())\n",
    "    text = ' '.join(\n",
    "        re.sub(\"gt; \", \" \", text).split())\n",
    "    # remove single char\n",
    "    text = ' '.join(\n",
    "        re.sub(r\"\\b[a-zA-Z]\\b\", \" \", text).split())\n",
    "    return text\n",
    "\n",
    "data['Text'] = data['Text'].apply(remove_comments_special)\n",
    "\n",
    "# remove symbol\n",
    "def remove_symbol(text):\n",
    "    text = ''.join(\n",
    "        re.sub(r\"[\\!\\@\\#\\$\\%\\^\\&\\*\\(\\)\\?\\,\\\"\\|\\:]+\", \"\", text)\n",
    "    )\n",
    "    return text\n",
    "\n",
    "data['Text'] = data['Text'].apply(remove_symbol)\n",
    "\n",
    "# remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "\n",
    "print('Cleaning Result : \\n')\n",
    "data.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "data['Text_Clean'] = data['Text'].apply(tokenize_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tokenizing Result : \\n')\n",
    "data.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering | Singkatan Indo\n",
    "normalizad_word = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/meisaputri21/Indonesian-Twitter-Emotion-Dataset/master/kamus_singkatan.csv\", sep=\";\", header=None)\n",
    "normalizad_word_dict = {}\n",
    "\n",
    "for index, row in normalizad_word.iterrows():\n",
    "    if row[0] not in normalizad_word_dict:\n",
    "        normalizad_word_dict[row[0]] = row[1]\n",
    "\n",
    "\n",
    "def normalized_term(document):\n",
    "    return [normalizad_word_dict[term] if term in normalizad_word_dict else term for term in document]\n",
    "\n",
    "\n",
    "data['Text_Clean'] = data['Text_Clean'].apply(normalized_term)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering | Stop Word\n",
    "list_stopwords = (['yang', 'untuk', 'pada', 'ke', 'para', 'namun', 'menurut', 'antara', 'seperti', 'jika', 'jika', 'sehingga', 'mungkin', 'kembali', 'dan', 'ini', 'karena', 'oleh', 'saat', 'sekitar', 'bagi', 'serta', 'di', 'dari', 'sebagai', 'hal', 'ketika', 'adalah', 'itu', 'dalam', 'bahwa', 'atau', 'dengan', 'akan', 'juga', 'kalau', 'ada', 'terhadap', 'secara', 'agar', 'lain', 'jadi', 'yang ', 'sudah', 'sudah begitu', 'mengapa', 'kenapa', 'yaitu', 'yakni', 'daripada', 'itulah', 'lagi', 'maka', 'tentang', 'demi', 'dimana', 'kemana', 'pula', 'sambil', 'sebelum', 'sesudah', 'supaya', 'guna', 'kah', 'pun', 'sampai', 'sedangkan', 'selagi',\n",
    "                  'sementara', 'tetapi', 'apakah', 'sebab', 'selain', 'seolah', 'seraya', 'seterusnya', 'dsb', 'dst', 'dll', 'dahulu', 'dulunya', 'anu', 'demikian', 'tapi', 'juga', 'mari', 'nanti', 'melainkan', 'oh', 'ok', 'sebetulnya', 'setiap', 'sesuatu', 'pasti', 'saja', 'toh', 'ya', 'walau', 'apalagi', 'bagaimanapun', 'yg', 'dg', 'rt', 'dgn', 'ny', 'd', 'klo', 'kalo', 'amp', 'biar', 'bikin', 'bilang', 'krn', 'nya', 'nih', 'sih', 'ah', 'ssh', 'om', 'ah', 'si', 'tau', 'tuh', 'utk', 'ya', 'cek', 'jd', 'aja', 't', 'nyg', 'hehe', 'pen', 'nan', 'loh', 'rt', '&amp', 'yah', 'ni', 'ret', 'za', 'nak', 'haa', 'zaa', 'maa', 'lg', 'eh', 'hmm', 'kali'])\n",
    "\n",
    "list_stopwords = set(list_stopwords)\n",
    "\n",
    "\n",
    "def stopwords_removal(words):\n",
    "    return [word for word in words if word not in list_stopwords]\n",
    "\n",
    "\n",
    "data['Text_Clean'] = data['Text_Clean'].apply(stopwords_removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sambungin_kata(text):\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "data['Text_Clean_Sambung'] = data['Text_Clean'].apply(sambungin_kata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['Text_Clean'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.1 Menejermah Tweet Clean hasil Preproc ke Bahasa Inggris** (Translating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install googletrans==4.0.0rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install google-cloud-translate==2.0.1\n",
    "%pip install --upgrade google-cloud-translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: JIKA MEMAKAI GOOGLE TRANSLATE API WEBSITE\n",
    "# import pandas as pd\n",
    "# from googletrans import Translator\n",
    "\n",
    "# translator = Translator()\n",
    "\n",
    "# translated_word = []\n",
    "# new_row = []\n",
    "\n",
    "# data = data.reset_index()  # make sure indexes pair with number of rows\n",
    "\n",
    "# for index, row in data.iterrows():\n",
    "#     new_row.append(row['Text_Clean_Sambung'])\n",
    "\n",
    "# for per_row in new_row:\n",
    "#     out = translator.translate(per_row, dest='en')\n",
    "#     translated_word.append(out.text)\n",
    "\n",
    "# data.insert(loc=len(data.columns),\n",
    "#             column=\"text_english\", value=translated_word)\n",
    "\n",
    "# print('Translating has completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: JIKA MEMAKAI GOOGLE CLOUD TRANSLATE API\n",
    "# import os\n",
    "\n",
    "# from google.cloud import translate_v2\n",
    "\n",
    "# os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = r\"serviceKey.json\"\n",
    "\n",
    "# translate_client = translate_v2.Client()\n",
    "\n",
    "# text = \"Saya siapa dan kamu dimana\"\n",
    "\n",
    "# target = \"en\"\n",
    "\n",
    "# output = translate_client.translate(text, target_language=target)\n",
    "\n",
    "# print(output)\n",
    "# print(output['translatedText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REAL IMPLEMENTATION\n",
    "import os\n",
    "\n",
    "from google.cloud import translate_v2\n",
    "\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = r\"serviceKey.json\"\n",
    "\n",
    "translate_client = translate_v2.Client()\n",
    "target = \"en\"\n",
    "\n",
    "translated_word = []\n",
    "new_row = []\n",
    "data = data.reset_index()  # make sure indexes pair with number of rows\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    new_row.append(row['Text_Clean_Sambung'])\n",
    "\n",
    "for per_row in new_row:\n",
    "    output = translate_client.translate(per_row, target_language=\"en\")\n",
    "    translated_word.append(output['translatedText'])\n",
    "\n",
    "data.insert(loc=len(data.columns),\n",
    "            column=\"text_english\", value=translated_word)\n",
    "\n",
    "print('Translating has completed!')\n",
    "\n",
    "# up to csv\n",
    "filename = \"english_tweets_covid_dataset_50k.csv\"\n",
    "data.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2 Stemming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('https://raw.githubusercontent.com/hzlnqodrey/projek-akhir-prak-ds-sentimen-analisis-twitter-covid19-nlp-bayes/main/data_csv/english_tweets_covid_dataset_2k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['index', 'Text'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hapuskan symbol &#39; > ganti menjadi (')\n",
    "# remove  &#39; symbol\n",
    "def remove_symbol(text):\n",
    "    text = ''.join(\n",
    "        re.sub(r\"[&#39;]+\", \"'\", text)\n",
    "    )\n",
    "    return text\n",
    "\n",
    "data['text_english'] = data['text_english'].apply(remove_symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "\n",
    "def stemming_data(text):\n",
    "    return ps.stem(text)\n",
    "\n",
    "data['text_english'] = data['text_english'].apply(stemming_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Sentiment Analysis dengan NLP (TextBlob)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tweet = list(data['text_english'])\n",
    "polaritas = 0\n",
    "\n",
    "tot_positif = tot_negatif = tot_netral = total = 0\n",
    "status = []\n",
    "\n",
    "for i, tweet in enumerate(data_tweet):\n",
    "    analysis = TextBlob(tweet)\n",
    "    polaritas += analysis.polarity\n",
    "\n",
    "    if analysis.sentiment.polarity > 0.0:\n",
    "        tot_positif += 1\n",
    "        status.append('Positif')\n",
    "    elif analysis.sentiment.polarity == 0.0:\n",
    "        tot_netral += 1\n",
    "        status.append('Netral')\n",
    "    else: \n",
    "        tot_negatif += 1\n",
    "        status.append('Negatif')\n",
    "        \n",
    "    total += 1\n",
    "\n",
    "print(f'Hasil Analisis Data:\\nPositif = {tot_positif}\\nNetral = {tot_netral}\\nNegatif = {tot_negatif}')\n",
    "print(f'\\nTotal Data : {total}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tambahkan status sentiment ke dataframe\n",
    "status = pd.DataFrame({'klasifikasi': status})\n",
    "data['klasifikasi'] = status\n",
    "data.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.1 Word Cloud**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "def plot_cloud(wordcloud):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(wordcloud) \n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = ' '.join([tweets for tweets in data['text_english']])\n",
    "wordcloud = WordCloud(width = 3000, height = 2000, random_state=3, background_color='white', colormap='Set2', collocations=False, stopwords = STOPWORDS).generate(all_words)\n",
    "plot_cloud(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = ' '.join([tweets for tweets in data['Text_Clean_Sambung']])\n",
    "wordcloud = WordCloud(width = 3000, height = 2000, random_state=3, background_color='white', colormap='Set2', collocations=False, stopwords = STOPWORDS).generate(all_words)\n",
    "plot_cloud(wordcloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.2 Pie Chart**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_pie(label, data, legend_title):\n",
    "    fig, ax = plt.subplots(figsize=(8, 10), subplot_kw=dict(aspect='equal'))\n",
    "\n",
    "    labels = [x.split()[-1] for x in label]\n",
    "\n",
    "    def func(pct, allvals):\n",
    "        absolute = int(pct/100.*np.sum(allvals))\n",
    "        return \"{:.1f}% ({:d})\".format(pct, absolute)\n",
    "\n",
    "    wedges, texts, autotexts = ax.pie(data, autopct=lambda pct: func(pct, data),\n",
    "                                      textprops=dict(color=\"w\"))\n",
    "\n",
    "    ax.legend(wedges, labels,\n",
    "              title=legend_title,\n",
    "              loc=\"center left\",\n",
    "              bbox_to_anchor=(1, 0, 0.5, 1))\n",
    "\n",
    "    plt.setp(autotexts, size=10, weight=\"bold\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ['Positif', 'Negatif', 'Netral']\n",
    "count_data = [tot_positif+1, tot_negatif+1, tot_netral]\n",
    "\n",
    "show_pie(label, count_data, \"Status\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kesimpulan :** \n",
    "\n",
    "Hasil dari Sentiment Analysis menggunakan NLP library TextBlob adalah: persentase sentimen masyarakat tentang menanggapi topik COVID-19 selama periode Oktober s.d. November yaitu 38% Positif, 35% Netral, dan 26% Negatif "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Klasifikasi Data Tweets dengan Naive Bayes** (Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.1 Drop unused columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.drop(['Tanggal', 'Username', 'Text_Clean_Sambung'], axis=1, inplace=False)\n",
    "dataset = [tuple(x) for x in dataset.to_records(index=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.2 Set random sample for training the NB Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "set_positif = []\n",
    "set_negatif = []\n",
    "set_netral = []\n",
    "\n",
    "for n in dataset:\n",
    "    if(n[1] == 'Positif'):\n",
    "        set_positif.append(n)\n",
    "    elif(n[1] == 'Negatif'):\n",
    "        set_negatif.append(n)\n",
    "    else:\n",
    "        set_netral.append(n)\n",
    "\n",
    "set_positif = random.sample(set_positif, k=int(len(set_positif)/2))\n",
    "set_negatif = random.sample(set_negatif, k=int(len(set_negatif)/2))\n",
    "set_netral = random.sample(set_netral, k=int(len(set_netral)/2))\n",
    "\n",
    "train = set_positif + set_negatif + set_netral\n",
    "\n",
    "train_set = []\n",
    "\n",
    "for n in train:\n",
    "    train_set.append(n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.3 The Naive Bayes Classifier Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "cl = NaiveBayesClassifier(train_set)\n",
    "print('Akurasi Test:', cl.accuracy(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.4 Sentiment Analysis on Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tweet = list(data['text_english'])\n",
    "polaritas = 0\n",
    "\n",
    "status = []\n",
    "tot_positif = tot_negatif = tot_netral = total = 0\n",
    "\n",
    "for i, tweet in enumerate(data_tweet):\n",
    "    analysis = TextBlob(tweet, classifier=cl)\n",
    "\n",
    "    if analysis.classify() == 'Positif':\n",
    "        tot_positif += 1\n",
    "    elif analysis.classify() == 'Netral':\n",
    "        tot_netral += 1\n",
    "    else:\n",
    "        tot_negatif += 1\n",
    "    \n",
    "    status.append(analysis.classify())\n",
    "    total += 1 \n",
    "    \n",
    "print(f'\\nHasil Analisis Data:\\nPositif = {tot_positif}\\nNetral = {tot_netral}\\nNegatif = {tot_negatif}')\n",
    "print(f'\\nTotal Data : {total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = pd.DataFrame({'klasifikasi_bayes': status})\n",
    "data['klasifikasi_bayes'] = status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ['Positif', 'Negatif', 'Netral']\n",
    "count_data = [tot_positif+1, tot_negatif+1, tot_netral]\n",
    "\n",
    "show_pie(label, count_data, \"Status\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kesimpulan:** \n",
    "\n",
    "Dengan Menggunakan Metode Naive Bayes Classifier Dengan Tingkat Akurasi 0.77 (77%) Maka :\n",
    "\n",
    "1.   Sentimen Positif : 707\n",
    "2.   Sentimen Netral : 885\n",
    "3.   Sentimen Negatif : 408\n",
    "\n",
    "Sehingga Masyarakat Rata-Rata Memiliki Sentimen Netral Terhadap Topik Covid 19 pada periode Oktober s.d. November 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Klasifikasi Yang Diubah Setelah Menggunakan Metode Naive Bayes Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_eval = [tuple(x) for x in data.to_records(index=False)]\n",
    "\n",
    "for n in data_eval:\n",
    "    if n[5] != n[6]:\n",
    "        print(f'Text: {n[3]}\\nClassifier: {n[5]}\\nClassifier Bayes: {n[6]} \\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "898a950986d43450680efc03f9903704e020e6e6b23d64c62a66308a081cc53c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
