{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Projek Akhir Praktikum Data Science**\n",
    "\n",
    "> Analisis Sentimen Mengenai Vaksin COVID-19 Di Indonesia Menggunakan Metode Naive Bayes Classifier dan NLP Pada Sosial Media Twitter\n",
    "\n",
    "**Oleh Kelompok 2 :**\n",
    "1. Hazlan Muhammad Qodri (123190080) @hzlnqodrey\n",
    "2. Elisia Dwi Rahayu (123190062) @elisiadwirahayu\n",
    "3. Shania Septika Inayasari (123190055) @shaniainayasari\n",
    "\n",
    "**Penjelasan Projek :**\n",
    "\n",
    "Adapun pada penelitian menekankan kepada sentimen masyarakat terhadap mengenai vaksin COVID-19. Proses analisisnya akan dilakukan berdasarkan tweet yang menyertakan tagar vaksin dan pencarian di twitter dengan keyword vaksin covid 19."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Scraping Data from Twitter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install snscrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get All Covid Sentiment Data from January 1st, 2020 until November 1st, 2022\n",
    "query = \"covid since:2020-01-01 until:2022-11-01 lang:id\"\n",
    "limit = 2000 # limit 50k rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "\n",
    "for tweet in sntwitter.TwitterSearchScraper(query).get_items():\n",
    "    if len(tweets) == limit:\n",
    "        break\n",
    "    else:\n",
    "        tweets.append([\n",
    "            tweet.date,\n",
    "            tweet.username,\n",
    "            tweet.content\n",
    "        ])\n",
    "\n",
    "filename = 'tweets_covid_dataset_2k_raw_noindex.csv'\n",
    "tweets_df = pd.DataFrame(tweets, columns=['Tanggal', 'Username', 'Text'])\n",
    "tweets_df.to_csv(filename, index=False)\n",
    "print('Scraping has completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Wrangling Data** (Preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tweet-preprocessor\n",
    "%pip install textblob\n",
    "%pip install wordcloud\n",
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessor as preproc\n",
    "from textblob import TextBlob\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import csv\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data from dataset\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/hzlnqodrey/projek-akhir-prak-ds-sentimen-analisis-twitter-covid19-nlp-bayes/main/data_csv/tweets_covid_dataset_2k_raw_noindex.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Case Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Text'] = data['Text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Case Folding Result : \\n')\n",
    "data.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning overall\n",
    "def preprocessing_data(x):\n",
    "    return preproc.clean(x)\n",
    "\n",
    "data['Text'] = data['Text'].apply(preprocessing_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cleaning Result : \\n')\n",
    "data.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning Result : \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tanggal</th>\n",
       "      <th>Username</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1416</th>\n",
       "      <td>2022-10-31 07:44:53+00:00</td>\n",
       "      <td>beritacovid</td>\n",
       "      <td>indonesia cabut status gawat darurat covid-19 ini penjelasan pemerintah - msn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>2022-10-31 14:32:35+00:00</td>\n",
       "      <td>kangebadut</td>\n",
       "      <td>oh mungkin berkurangnya krn bnyk yg hilang dri muka bumi krn covid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1485</th>\n",
       "      <td>2022-10-31 07:25:16+00:00</td>\n",
       "      <td>abuhafiz1</td>\n",
       "      <td>jangan la ulangi kesilapan dulu ambil alih lebuhraya ni mahalbeban kewangan negara kita banyak hutang lepas covid-19 janganlah selalu populis panjangkan konsesi dan rendahkan harga tol mungkin alternatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>2022-10-31 10:32:37+00:00</td>\n",
       "      <td>dindaasyafiraa</td>\n",
       "      <td>plis jangan covid lagi plis plis sedih banget nanti sia sia persiapan ini</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>2022-10-31 10:31:03+00:00</td>\n",
       "      <td>radiosushifm</td>\n",
       "      <td>sheila on kembali hibur sheilagank lg ni sushimitra setelah kabar hiatu sejak pandemi covid tpi netizen malah fokus ke om duta nya yg tak menua  hmmm gimana mnurut kmu sushimitra  xixi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Tanggal        Username                                                                                                                                                                                                         Text\n",
       "1416  2022-10-31 07:44:53+00:00     beritacovid                                                                                                                                indonesia cabut status gawat darurat covid-19 ini penjelasan pemerintah - msn\n",
       "469   2022-10-31 14:32:35+00:00      kangebadut                                                                                                                                           oh mungkin berkurangnya krn bnyk yg hilang dri muka bumi krn covid\n",
       "1485  2022-10-31 07:25:16+00:00       abuhafiz1  jangan la ulangi kesilapan dulu ambil alih lebuhraya ni mahalbeban kewangan negara kita banyak hutang lepas covid-19 janganlah selalu populis panjangkan konsesi dan rendahkan harga tol mungkin alternatif\n",
       "994   2022-10-31 10:32:37+00:00  dindaasyafiraa                                                                                                                                    plis jangan covid lagi plis plis sedih banget nanti sia sia persiapan ini\n",
       "998   2022-10-31 10:31:03+00:00    radiosushifm                     sheila on kembali hibur sheilagank lg ni sushimitra setelah kabar hiatu sejak pandemi covid tpi netizen malah fokus ke om duta nya yg tak menua  hmmm gimana mnurut kmu sushimitra  xixi"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cleaning remove_comments_special\n",
    "def remove_comments_special(text):\n",
    "    # remove tab, new line, and back slice\n",
    "    text = text.replace('\\\\t', \" \").replace('\\\\n', \" \").replace(\n",
    "        '\\\\u', \" \").replace('\\\\', \" \").replace('.', \" \")\n",
    "    # remove non ASCII (emoticon, chinese word, .etc)\n",
    "    text = text.encode('ascii', 'replace').decode('ascii')\n",
    "    # remove mention, link, hashtag\n",
    "    text = ' '.join(\n",
    "        re.sub(\"([@#][A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\", \" \", text).split())\n",
    "    # remove ascii decoded\n",
    "    text = ' '.join(\n",
    "        re.sub(\"amp; \", \" \", text).split())\n",
    "    text = ' '.join(\n",
    "        re.sub(\"lt; \", \" \", text).split())\n",
    "    text = ' '.join(\n",
    "        re.sub(\"gt; \", \" \", text).split())\n",
    "    # remove single char\n",
    "    text = ' '.join(\n",
    "        re.sub(r\"\\b[a-zA-Z]\\b\", \" \", text).split())\n",
    "    return text\n",
    "\n",
    "data['Text'] = data['Text'].apply(remove_comments_special)\n",
    "\n",
    "# remove symbol\n",
    "def remove_symbol(text):\n",
    "    text = ''.join(\n",
    "        re.sub(r\"[\\!\\@\\#\\$\\%\\^\\&\\*\\(\\)\\?\\,\\\"\\|\\:]+\", \"\", text)\n",
    "    )\n",
    "    return text\n",
    "\n",
    "data['Text'] = data['Text'].apply(remove_symbol)\n",
    "\n",
    "# remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "\n",
    "print('Cleaning Result : \\n')\n",
    "data.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "data['Text_Clean'] = data['Text'].apply(tokenize_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing Result : \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tanggal</th>\n",
       "      <th>Username</th>\n",
       "      <th>Text</th>\n",
       "      <th>Text_Clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1685</th>\n",
       "      <td>2022-10-31 06:08:16+00:00</td>\n",
       "      <td>KuretaID</td>\n",
       "      <td>cerita nabilah ayu raih hidayah kenakan hijab di tengah pandemi covid-19</td>\n",
       "      <td>[cerita, nabilah, ayu, raih, hidayah, kenakan, hijab, di, tengah, pandemi, covid-19]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1694</th>\n",
       "      <td>2022-10-31 06:04:32+00:00</td>\n",
       "      <td>Sugirama</td>\n",
       "      <td>covid-19 terkendali djbc ungkap nasib insentif fiskal vaksin dan alkes</td>\n",
       "      <td>[covid-19, terkendali, djbc, ungkap, nasib, insentif, fiskal, vaksin, dan, alkes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1429</th>\n",
       "      <td>2022-10-31 07:43:07+00:00</td>\n",
       "      <td>Linda_Christi</td>\n",
       "      <td>awas efek samping covid-19 padawanita</td>\n",
       "      <td>[awas, efek, samping, covid-19, padawanita]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>2022-10-31 17:38:24+00:00</td>\n",
       "      <td>Acidicus</td>\n",
       "      <td>bwahahahahahahahahno</td>\n",
       "      <td>[bwahahahahahahahahno]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>2022-10-31 14:32:10+00:00</td>\n",
       "      <td>CariTahuPasti</td>\n",
       "      <td>belakangan ini banyak kejadian yg memakan korban hingga ratusan org mulai dr kanjuruhan korea selatan india  lainnya salah satu faktornya mungkin orgg2 telah lelah dgn pandemi yg sdh mau thn sehingga byk berkumpultetap hati-hati yaingat jaga prokes  covid masih ada</td>\n",
       "      <td>[belakangan, ini, banyak, kejadian, yg, memakan, korban, hingga, ratusan, org, mulai, dr, kanjuruhan, korea, selatan, india, lainnya, salah, satu, faktornya, mungkin, orgg2, telah, lelah, dgn, pandemi, yg, sdh, mau, thn, sehingga, byk, berkumpultetap, hati-hati, yaingat, jaga, prokes, covid, masih, ada]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Tanggal       Username                                                                                                                                                                                                                                                                       Text                                                                                                                                                                                                                                                                                                        Text_Clean\n",
       "1685  2022-10-31 06:08:16+00:00       KuretaID                                                                                                                                                                                                   cerita nabilah ayu raih hidayah kenakan hijab di tengah pandemi covid-19                                                                                                                                                                                                                              [cerita, nabilah, ayu, raih, hidayah, kenakan, hijab, di, tengah, pandemi, covid-19]\n",
       "1694  2022-10-31 06:04:32+00:00       Sugirama                                                                                                                                                                                                     covid-19 terkendali djbc ungkap nasib insentif fiskal vaksin dan alkes                                                                                                                                                                                                                                 [covid-19, terkendali, djbc, ungkap, nasib, insentif, fiskal, vaksin, dan, alkes]\n",
       "1429  2022-10-31 07:43:07+00:00  Linda_Christi                                                                                                                                                                                                                                      awas efek samping covid-19 padawanita                                                                                                                                                                                                                                                                       [awas, efek, samping, covid-19, padawanita]\n",
       "190   2022-10-31 17:38:24+00:00       Acidicus                                                                                                                                                                                                                                                       bwahahahahahahahahno                                                                                                                                                                                                                                                                                            [bwahahahahahahahahno]\n",
       "471   2022-10-31 14:32:10+00:00  CariTahuPasti  belakangan ini banyak kejadian yg memakan korban hingga ratusan org mulai dr kanjuruhan korea selatan india  lainnya salah satu faktornya mungkin orgg2 telah lelah dgn pandemi yg sdh mau thn sehingga byk berkumpultetap hati-hati yaingat jaga prokes  covid masih ada  [belakangan, ini, banyak, kejadian, yg, memakan, korban, hingga, ratusan, org, mulai, dr, kanjuruhan, korea, selatan, india, lainnya, salah, satu, faktornya, mungkin, orgg2, telah, lelah, dgn, pandemi, yg, sdh, mau, thn, sehingga, byk, berkumpultetap, hati-hati, yaingat, jaga, prokes, covid, masih, ada]"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Tokenizing Result : \\n')\n",
    "data.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering | Singkatan Indo\n",
    "normalizad_word = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/meisaputri21/Indonesian-Twitter-Emotion-Dataset/master/kamus_singkatan.csv\", sep=\";\", header=None)\n",
    "normalizad_word_dict = {}\n",
    "\n",
    "for index, row in normalizad_word.iterrows():\n",
    "    if row[0] not in normalizad_word_dict:\n",
    "        normalizad_word_dict[row[0]] = row[1]\n",
    "\n",
    "\n",
    "def normalized_term(document):\n",
    "    return [normalizad_word_dict[term] if term in normalizad_word_dict else term for term in document]\n",
    "\n",
    "\n",
    "data['Text_Clean'] = data['Text_Clean'].apply(normalized_term)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering | Stop Word\n",
    "list_stopwords = (['yang', 'untuk', 'pada', 'ke', 'para', 'namun', 'menurut', 'antara', 'seperti', 'jika', 'jika', 'sehingga', 'mungkin', 'kembali', 'dan', 'ini', 'karena', 'oleh', 'saat', 'sekitar', 'bagi', 'serta', 'di', 'dari', 'sebagai', 'hal', 'ketika', 'adalah', 'itu', 'dalam', 'bahwa', 'atau', 'dengan', 'akan', 'juga', 'kalau', 'ada', 'terhadap', 'secara', 'agar', 'lain', 'jadi', 'yang ', 'sudah', 'sudah begitu', 'mengapa', 'kenapa', 'yaitu', 'yakni', 'daripada', 'itulah', 'lagi', 'maka', 'tentang', 'demi', 'dimana', 'kemana', 'pula', 'sambil', 'sebelum', 'sesudah', 'supaya', 'guna', 'kah', 'pun', 'sampai', 'sedangkan', 'selagi',\n",
    "                  'sementara', 'tetapi', 'apakah', 'sebab', 'selain', 'seolah', 'seraya', 'seterusnya', 'dsb', 'dst', 'dll', 'dahulu', 'dulunya', 'anu', 'demikian', 'tapi', 'juga', 'mari', 'nanti', 'melainkan', 'oh', 'ok', 'sebetulnya', 'setiap', 'sesuatu', 'pasti', 'saja', 'toh', 'ya', 'walau', 'apalagi', 'bagaimanapun', 'yg', 'dg', 'rt', 'dgn', 'ny', 'd', 'klo', 'kalo', 'amp', 'biar', 'bikin', 'bilang', 'krn', 'nya', 'nih', 'sih', 'ah', 'ssh', 'om', 'ah', 'si', 'tau', 'tuh', 'utk', 'ya', 'cek', 'jd', 'aja', 't', 'nyg', 'hehe', 'pen', 'nan', 'loh', 'rt', '&amp', 'yah', 'ni', 'ret', 'za', 'nak', 'haa', 'zaa', 'maa', 'lg', 'eh', 'hmm', 'kali'])\n",
    "\n",
    "list_stopwords = set(list_stopwords)\n",
    "\n",
    "\n",
    "def stopwords_removal(words):\n",
    "    return [word for word in words if word not in list_stopwords]\n",
    "\n",
    "\n",
    "data['Text_Clean'] = data['Text_Clean'].apply(stopwords_removal)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.1 Menejermah Tweet Clean hasil Preproc ke Bahasa Inggris** (Translating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install googletrans==4.0.0rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the truth of light\n"
     ]
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "\n",
    "translate = Translator()\n",
    "\n",
    "out = translate.translate('veritas lux', dest='en')\n",
    "\n",
    "print(out.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2 Stemming**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Modelling Data** (Modeling and Training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Klasifikasi Data dengan Naive Bayes** (Classification)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "898a950986d43450680efc03f9903704e020e6e6b23d64c62a66308a081cc53c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
