{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Projek Akhir Praktikum Data Science**\n",
    "\n",
    "> Analisis Sentimen Mengenai Vaksin COVID-19 Di Indonesia Menggunakan Metode Naive Bayes Classifier dan NLP Pada Sosial Media Twitter\n",
    "\n",
    "**Oleh Kelompok 2 :**\n",
    "1. Hazlan Muhammad Qodri (123190080) @hzlnqodrey\n",
    "2. Elisia Dwi Rahayu (123190062) @elisiadwirahayu\n",
    "3. Shania Septika Inayasari (123190055) @shaniainayasari\n",
    "\n",
    "**Penjelasan Projek :**\n",
    "\n",
    "Adapun pada penelitian menekankan kepada sentimen masyarakat terhadap mengenai vaksin COVID-19. Proses analisisnya akan dilakukan berdasarkan tweet yang menyertakan tagar vaksin dan pencarian di twitter dengan keyword vaksin covid 19."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Scraping Data from Twitter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install snscrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get All Covid Sentiment Data from January 1st, 2020 until November 1st, 2022\n",
    "query = \"covid since:2020-01-01 until:2022-11-01 lang:id\"\n",
    "limit = 2000 # limit 50k rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "\n",
    "for tweet in sntwitter.TwitterSearchScraper(query).get_items():\n",
    "    if len(tweets) == limit:\n",
    "        break\n",
    "    else:\n",
    "        tweets.append([\n",
    "            tweet.date,\n",
    "            tweet.username,\n",
    "            tweet.content\n",
    "        ])\n",
    "\n",
    "filename = 'tweets_covid_dataset_2k_raw_noindex.csv'\n",
    "tweets_df = pd.DataFrame(tweets, columns=['Tanggal', 'Username', 'Text'])\n",
    "tweets_df.to_csv(filename, index=False)\n",
    "print('Scraping has completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Wrangling Data** (Preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tweet-preprocessor\n",
    "%pip install textblob\n",
    "%pip install wordcloud\n",
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessor as preproc\n",
    "from textblob import TextBlob\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import csv\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data from dataset\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/hzlnqodrey/projek-akhir-prak-ds-sentimen-analisis-twitter-covid19-nlp-bayes/main/data_csv/tweets_covid_dataset_2k_raw_noindex.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Case Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Text'] = data['Text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Case Folding Result : \\n')\n",
    "data.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning overall\n",
    "def preprocessing_data(x):\n",
    "    return preproc.clean(x)\n",
    "\n",
    "data['Text'] = data['Text'].apply(preprocessing_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cleaning Result : \\n')\n",
    "data.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning remove_comments_special\n",
    "def remove_comments_special(text):\n",
    "    # remove tab, new line, and back slice\n",
    "    text = text.replace('\\\\t', \" \").replace('\\\\n', \" \").replace(\n",
    "        '\\\\u', \" \").replace('\\\\', \" \").replace('.', \" \")\n",
    "    # remove non ASCII (emoticon, chinese word, .etc)\n",
    "    text = text.encode('ascii', 'replace').decode('ascii')\n",
    "    # remove mention, link, hashtag\n",
    "    text = ' '.join(\n",
    "        re.sub(\"([@#][A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\", \" \", text).split())\n",
    "    # remove ascii decoded\n",
    "    text = ' '.join(\n",
    "        re.sub(\"amp; \", \" \", text).split())\n",
    "    text = ' '.join(\n",
    "        re.sub(\"lt; \", \" \", text).split())\n",
    "    text = ' '.join(\n",
    "        re.sub(\"gt; \", \" \", text).split())\n",
    "    # remove single char\n",
    "    text = ' '.join(\n",
    "        re.sub(r\"\\b[a-zA-Z]\\b\", \" \", text).split())\n",
    "    return text\n",
    "\n",
    "data['Text'] = data['Text'].apply(remove_comments_special)\n",
    "\n",
    "# remove symbol\n",
    "def remove_symbol(text):\n",
    "    text = ''.join(\n",
    "        re.sub(r\"[\\!\\@\\#\\$\\%\\^\\&\\*\\(\\)\\?\\,\\\"\\|\\:]+\", \"\", text)\n",
    "    )\n",
    "    return text\n",
    "\n",
    "data['Text'] = data['Text'].apply(remove_symbol)\n",
    "\n",
    "# remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "\n",
    "print('Cleaning Result : \\n')\n",
    "data.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "data['Text_Clean'] = data['Text'].apply(tokenize_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tokenizing Result : \\n')\n",
    "data.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering | Singkatan Indo\n",
    "normalizad_word = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/meisaputri21/Indonesian-Twitter-Emotion-Dataset/master/kamus_singkatan.csv\", sep=\";\", header=None)\n",
    "normalizad_word_dict = {}\n",
    "\n",
    "for index, row in normalizad_word.iterrows():\n",
    "    if row[0] not in normalizad_word_dict:\n",
    "        normalizad_word_dict[row[0]] = row[1]\n",
    "\n",
    "\n",
    "def normalized_term(document):\n",
    "    return [normalizad_word_dict[term] if term in normalizad_word_dict else term for term in document]\n",
    "\n",
    "\n",
    "data['Text_Clean'] = data['Text_Clean'].apply(normalized_term)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering | Stop Word\n",
    "list_stopwords = (['yang', 'untuk', 'pada', 'ke', 'para', 'namun', 'menurut', 'antara', 'seperti', 'jika', 'jika', 'sehingga', 'mungkin', 'kembali', 'dan', 'ini', 'karena', 'oleh', 'saat', 'sekitar', 'bagi', 'serta', 'di', 'dari', 'sebagai', 'hal', 'ketika', 'adalah', 'itu', 'dalam', 'bahwa', 'atau', 'dengan', 'akan', 'juga', 'kalau', 'ada', 'terhadap', 'secara', 'agar', 'lain', 'jadi', 'yang ', 'sudah', 'sudah begitu', 'mengapa', 'kenapa', 'yaitu', 'yakni', 'daripada', 'itulah', 'lagi', 'maka', 'tentang', 'demi', 'dimana', 'kemana', 'pula', 'sambil', 'sebelum', 'sesudah', 'supaya', 'guna', 'kah', 'pun', 'sampai', 'sedangkan', 'selagi',\n",
    "                  'sementara', 'tetapi', 'apakah', 'sebab', 'selain', 'seolah', 'seraya', 'seterusnya', 'dsb', 'dst', 'dll', 'dahulu', 'dulunya', 'anu', 'demikian', 'tapi', 'juga', 'mari', 'nanti', 'melainkan', 'oh', 'ok', 'sebetulnya', 'setiap', 'sesuatu', 'pasti', 'saja', 'toh', 'ya', 'walau', 'apalagi', 'bagaimanapun', 'yg', 'dg', 'rt', 'dgn', 'ny', 'd', 'klo', 'kalo', 'amp', 'biar', 'bikin', 'bilang', 'krn', 'nya', 'nih', 'sih', 'ah', 'ssh', 'om', 'ah', 'si', 'tau', 'tuh', 'utk', 'ya', 'cek', 'jd', 'aja', 't', 'nyg', 'hehe', 'pen', 'nan', 'loh', 'rt', '&amp', 'yah', 'ni', 'ret', 'za', 'nak', 'haa', 'zaa', 'maa', 'lg', 'eh', 'hmm', 'kali'])\n",
    "\n",
    "list_stopwords = set(list_stopwords)\n",
    "\n",
    "\n",
    "def stopwords_removal(words):\n",
    "    return [word for word in words if word not in list_stopwords]\n",
    "\n",
    "\n",
    "data['Text_Clean'] = data['Text_Clean'].apply(stopwords_removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sambungin_kata(text):\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "data['Text_Clean_Sambung'] = data['Text_Clean'].apply(sambungin_kata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['Text_Clean'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.1 Menejermah Tweet Clean hasil Preproc ke Bahasa Inggris** (Translating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install googletrans==4.0.0rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-translate==2.0.1 in c:\\users\\hazlan m qodri\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.1.0 in c:\\users\\hazlan m qodri\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-cloud-translate==2.0.1) (1.7.3)\n",
      "Requirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.15.0 in c:\\users\\hazlan m qodri\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-cloud-translate==2.0.1) (1.33.2)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in c:\\users\\hazlan m qodri\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1) (2.27.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in c:\\users\\hazlan m qodri\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1) (1.57.0)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in c:\\users\\hazlan m qodri\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1) (1.35.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5 in c:\\users\\hazlan m qodri\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1) (3.20.3)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in c:\\users\\hazlan m qodri\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1) (1.48.2)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\users\\hazlan m qodri\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1) (1.50.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\hazlan m qodri\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-cloud-core<2.0dev,>=1.1.0->google-cloud-translate==2.0.1) (1.15.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\hazlan m qodri\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1) (0.2.8)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in c:\\users\\hazlan m qodri\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1) (41.2.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\hazlan m qodri\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1) (4.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\hazlan m qodri\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1) (4.2.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hazlan m qodri\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hazlan m qodri\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\hazlan m qodri\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hazlan m qodri\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1) (2.10)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\hazlan m qodri\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.15.0->google-cloud-translate==2.0.1) (0.4.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-translate in c:\\users\\hazlan m qodri\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (2.0.1)\n",
      "Collecting google-cloud-translate\n",
      "  Downloading google_cloud_translate-3.8.4-py2.py3-none-any.whl (117 kB)\n",
      "     -------------------------------------- 117.6/117.6 kB 1.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in c:\\users\\hazlan m qodri\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-cloud-translate) (3.20.3)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.0\n",
      "  Downloading proto_plus-1.22.1-py3-none-any.whl (47 kB)\n",
      "     ---------------------------------------- 47.9/47.9 kB 1.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in c:\\users\\hazlan m qodri\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-cloud-translate) (1.33.2)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.3.0 in c:\\users\\hazlan m qodri\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-cloud-translate) (1.7.3)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in c:\\users\\hazlan m qodri\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-translate) (1.35.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in c:\\users\\hazlan m qodri\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-translate) (2.27.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in c:\\users\\hazlan m qodri\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-translate) (1.57.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\users\\hazlan m qodri\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-translate) (1.50.0)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in c:\\users\\hazlan m qodri\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-translate) (1.48.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\hazlan m qodri\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-cloud-core<3.0.0dev,>=1.3.0->google-cloud-translate) (1.15.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\hazlan m qodri\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-translate) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\hazlan m qodri\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-translate) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\hazlan m qodri\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-translate) (4.8)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in c:\\users\\hazlan m qodri\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-translate) (41.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hazlan m qodri\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-translate) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hazlan m qodri\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-translate) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\hazlan m qodri\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-translate) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hazlan m qodri\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-translate) (2.10)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\hazlan m qodri\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-translate) (0.4.8)\n",
      "Installing collected packages: proto-plus, google-cloud-translate\n",
      "  Attempting uninstall: google-cloud-translate\n",
      "    Found existing installation: google-cloud-translate 2.0.1\n",
      "    Uninstalling google-cloud-translate-2.0.1:\n",
      "      Successfully uninstalled google-cloud-translate-2.0.1\n",
      "Successfully installed google-cloud-translate-3.8.4 proto-plus-1.22.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install google-cloud-translate==2.0.1\n",
    "%pip install --upgrade google-cloud-translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: JIKA MEMAKAI GOOGLE TRANSLATE API WEBSITE\n",
    "import pandas as pd\n",
    "from googletrans import Translator\n",
    "\n",
    "translator = Translator()\n",
    "\n",
    "translated_word = []\n",
    "new_row = []\n",
    "\n",
    "data = data.reset_index()  # make sure indexes pair with number of rows\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    new_row.append(row['Text_Clean_Sambung'])\n",
    "\n",
    "for per_row in new_row:\n",
    "    out = translator.translate(per_row, dest='en')\n",
    "    translated_word.append(out.text)\n",
    "\n",
    "data.insert(loc=len(data.columns),\n",
    "            column=\"text_english\", value=translated_word)\n",
    "\n",
    "print('Translating has completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'translatedText': 'Who am I and where are you', 'detectedSourceLanguage': 'id', 'input': 'Saya siapa dan kamu dimana'}\n",
      "Who am I and where are you\n"
     ]
    }
   ],
   "source": [
    "## NOTE: JIKA MEMAKAI GOOGLE CLOUD TRANSLATE API\n",
    "import os\n",
    "\n",
    "from google.cloud import translate_v2\n",
    "\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = r\"serviceKey.json\"\n",
    "\n",
    "translate_client = translate_v2.Client()\n",
    "\n",
    "text = \"Saya siapa dan kamu dimana\"\n",
    "\n",
    "target = \"en\"\n",
    "\n",
    "output = translate_client.translate(text, target_language=target)\n",
    "\n",
    "print(output)\n",
    "print(output['translatedText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# up to csv\n",
    "filename = \"english_tweets_covid_dataset_2k.csv\"\n",
    "data.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2.2 Stemming**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Modelling Data** (Modeling and Training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Klasifikasi Data dengan Naive Bayes** (Classification)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "898a950986d43450680efc03f9903704e020e6e6b23d64c62a66308a081cc53c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
